# -*- coding: utf-8 -*-
"""RandomWalk Data Science Assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IEtl33dmPbqjOSItrnniML-vSkfl_JUz
"""

!git clone https://github.com/Murahariah/randomwalk-ds-assessment-level2.git

import pandas as pd
import numpy as np

# Load the dataset
data_path = "/content/randomwalk-ds-assessment-level2/dataset.csv"
data = pd.read_csv(data_path)

"""**Q1: Identify missing or incorrect data in the dataset and apply appropriate preprocessing steps to clean it (code and explanation)**

**Answer:**

The dataset contains missing values in various columns, identified using a heatmap. To handle these missing values, I performed the following preprocessing steps:

Categorical Columns: For the 'sex' column, which contains 'unknown' as a placeholder, I replaced it with NaN and then filled missing values using the mode (most frequent value) of the column.
Numerical Columns: For other columns with missing numerical values, I filled them with the mean of the respective columns.
After these steps, I verified that no missing values remained in the dataset, ensuring data completeness for further analysis.
"""

data.head(10)

data.tail(10)

# data inspection
print("Initial Data Overview:\n", data.info())

data.isnull().sum()

print("Unique values before cleaning:", data['sex'].unique())

# Replace 'unknown' with NaN
data['sex'] = data['sex'].replace('unknown', np.nan)

data.tail(10)

print("Unique values before cleaning:", data['sex'].unique())

data.isnull().sum()

import pandas as pd
import plotly.express as px

missing_values = data.isnull()

fig = px.imshow(
    missing_values,
    labels=dict(x="Columns", y="Rows", color="Missing"),
    color_continuous_scale="Viridis",
    title="Heatmap of Missing Values"
)

fig.update_layout(
    xaxis=dict(tickangle=45, title="Columns"),
    yaxis=dict(title="Rows"),
    coloraxis_colorbar=dict(title="Missing")
)

fig.show()

data.isnull().mean().round(4)*100

for column in data.columns:
    if data[column].isnull().sum() > 0:  # Check for missing values
        if data[column].dtype == 'object':  # For categorical columns
            data[column] = data[column].fillna(data[column].mode()[0])
        else:  # For numerical columns
            data[column] = data[column].fillna(data[column].mean())

print(data.isnull().sum())

data.tail()

"""**Q2: What is the average body_mass_g for Gentoo penguins? (code)**"""

# Filter the dataset for Gentoo penguins
gentoo_data = data[data['species'] == 'Gentoo']

# Calculate the average body_mass_g for Gentoo penguins
average_body_mass_g = gentoo_data['body_mass_g'].mean()

# Print the result
print(f"The average body_mass_g for Gentoo penguins is: {average_body_mass_g}")

"""**Q3: How do the distributions of bill_length_mm and bill_depth_mm differ between the three penguin species? Analyze the skewness and kurtosis of each feature for different species. (code and explanation)**

**Answer**

1. **Skewness and Kurtosis Calculation**:
   - Skewness and kurtosis were calculated for bill_length_mm and bill_depth_mm for each species using the scipy.stats.skew and scipy.stats.kurtosis functions.

2. **Visualization**:
   - Histograms and density plots were created to visualize the distributions of these features by species.

### Findings:

- **Bill Length (bill_length_mm)**:
  - **Adelie**: Positive skew (rightward) with platykurtic distribution (light tails).
  - **Chinstrap**: Slight negative skew with platykurtic distribution.
  - **Gentoo**: Positive skew (rightward) with leptokurtic distribution (heavier tails).

- **Bill Depth (bill_depth_mm)**:
  - **Adelie**: Positive skew (rightward) with platykurtic distribution.
  - **Chinstrap**: Slight negative skew with platykurtic distribution.
  - **Gentoo**: Negative skew (leftward) with platykurtic distribution.
"""

import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import skew, kurtosis
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate skewness and kurtosis for bill_length_mm and bill_depth_mm by species
stats_summary = data.groupby('species').agg(
    bill_length_skewness=('bill_length_mm', lambda x: skew(x)),
    bill_length_kurtosis=('bill_length_mm', lambda x: kurtosis(x)),
    bill_depth_skewness=('bill_depth_mm', lambda x: skew(x)),
    bill_depth_kurtosis=('bill_depth_mm', lambda x: kurtosis(x))
).reset_index()

print(stats_summary)

fig1 = px.histogram(data, x='bill_length_mm', color='species',
                    title='Distribution of Bill Length by Species',
                    labels={'bill_length_mm': 'Bill Length (mm)', 'count': 'Count'},
                    nbins=30)
fig1.update_layout(barmode='overlay')
fig1.update_traces(opacity=0.5)
fig1.show()


fig2 = px.histogram(data, x='bill_depth_mm', color='species',
                    title='Distribution of Bill Depth by Species',
                    labels={'bill_depth_mm': 'Bill Depth (mm)', 'count': 'Count'},
                    nbins=30)
fig2.update_layout(barmode='overlay')
fig2.update_traces(opacity=0.5)
fig2.show()

plt.figure(figsize=(10, 6))
sns.kdeplot(data=data, x='bill_length_mm', hue='species', fill=True, common_norm=False,
            palette='Set1', linewidth=2)
plt.title('Density Curve of Bill Length by Species')
plt.xlabel('Bill Length (mm)')
plt.ylabel('Density')
plt.show()

plt.figure(figsize=(10, 6))
sns.kdeplot(data=data, x='bill_depth_mm', hue='species', fill=True, common_norm=False,
            palette='Set1', linewidth=2)
plt.title('Density Curve of Bill Depth by Species')
plt.xlabel('Bill Depth (mm)')
plt.ylabel('Density')
plt.show()

"""**Q4: Identify which features in the dataset have outliers. Provide the method used to detect them and visualize the outliers. (code and explanation)**

**Answer:**

Outliers in the dataset were detected using the **IQR (Interquartile Range)** method. For each numerical column, the IQR was calculated as the difference between the 75th percentile (Q3) and the 25th percentile (Q1). The lower and upper bounds were determined as:

- **Lower Bound:** \( Q1 - 1.5 * IQR \)
- **Upper Bound:** \( Q3 + 1.5 * IQR \)

Any data points outside these bounds were considered outliers. Specifically, the **'body_mass_g'** feature had outliers, which were detected and visualized through a **boxplot**. After identifying these outliers, they were removed from the dataset to ensure cleaner data for further analysis.

After removing outliers from the **'body_mass_g'** column, a final boxplot was plotted to confirm that the outliers were effectively eliminated.
"""

import plotly.express as px

# Numerical columns to visualize
columns_to_plot = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']

for col in columns_to_plot:
    fig = px.box(data, y=col, title=f"Boxplot for {col}", template='plotly', width=500, height=400)
    fig.update_layout(
        yaxis_title="Values",
        xaxis_title="",
    )
    fig.show()

import plotly.express as px
import pandas as pd

# Numerical columns to visualize
columns_to_plot = ['body_mass_g']

# Function to remove outliers using IQR method
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Remove outliers for each numerical column
for col in columns_to_plot:
    data = remove_outliers(data, col)

    # Generate individual boxplots
    fig = px.box(data, y=col, title=f"Boxplot for {col}", template='plotly', width=500, height=400)
    fig.update_layout(
        yaxis_title="Values",
        xaxis_title="",
    )
    fig.show()

"""**Q5: Does this dataset contribute to the curse of dimensionality? If yes perform PCA. (code and explanation required)**

The dataset includes 7 columns, which is not large enough to contribute to the curse of dimensionality. The curse typically affects datasets with high-dimensional feature spaces, making models more prone to overfitting and computational inefficiency. Since the number of features is small (7), this dataset does not face such issues, and applying PCA is not necessary.

Therefore, there is no contribution to the curse of dimensionality in this case.

**Q6: Use bill_length_mm vs bill_depth_mm and plot 7 different graphs to visualize them. (code)**

**1. Scatter Plot**
"""

import plotly.express as px

fig = px.scatter(
    data,
    x="bill_length_mm",
    y="bill_depth_mm",
    color="species",
    title="Bill Length vs. Bill Depth by Species",
    labels={"bill_length_mm": "Bill Length (mm)", "bill_depth_mm": "Bill Depth (mm)"}
)
fig.update_traces(marker=dict(size=10, opacity=0.7))
fig.show()

"""**2. Box Plot**"""

fig = px.box(
    data,
    x="bill_length_mm",
    y="bill_depth_mm",
    color="species",
    title="Box Plot of Bill Depth by Bill Length",
    labels={"bill_length_mm": "Bill Length (mm)", "bill_depth_mm": "Bill Depth (mm)"}
)
fig.show()

"""**3. Faceted Scatter Plot**"""

fig = px.scatter(
    data,
    x="bill_length_mm",
    y="bill_depth_mm",
    color="species",
    facet_col="species",
    title="Faceted Scatter Plot of Bill Length vs. Bill Depth",
    labels={"bill_length_mm": "Bill Length (mm)", "bill_depth_mm": "Bill Depth (mm)"}
)
fig.show()

"""**4. Scatter Plot with Linear Regression Lines**"""

fig = px.scatter(
    data,
    x="bill_length_mm",
    y="bill_depth_mm",
    color="species",
    labels={"bill_depth_mm": "Bill Depth (mm)", "bill_length_mm": "Bill Length (mm)", "species": "Species"},
    trendline="ols"
)

fig.update_layout(title="Scatter Plot with Linear Regression Lines")
fig.show()

"""**5. Heatmap**"""

import plotly.graph_objects as go
import numpy as np

heatmap, x_edges, y_edges = np.histogram2d(
    data["bill_length_mm"], data["bill_depth_mm"], bins=30
)

fig = go.Figure(
    go.Heatmap(
        z=heatmap.T,
        x=x_edges,
        y=y_edges,
        colorscale="Blues",
        colorbar_title="Count"
    )
)
fig.update_layout(
    title="Heatmap of Bill Length vs. Bill Depth",
    xaxis_title="Bill Length (mm)",
    yaxis_title="Bill Depth (mm)",
)
fig.show()

"""**6. Violin Plot**"""

fig = px.violin(
    data,
    x="bill_length_mm",
    y="bill_depth_mm",
    color="species",
    box=True,
    points="all",
    title="Violin Plot of Bill Depth by Bill Length",
    labels={"bill_length_mm": "Bill Length (mm)", "bill_depth_mm": "Bill Depth (mm)"}
)
fig.show()

"""**7. Pair Plot with Scatter and Density**"""

import plotly.express as px

fig = px.scatter_matrix(
    data,
    dimensions=["bill_length_mm", "bill_depth_mm"],
    color="species",
    labels={
        "bill_length_mm": "Bill Length (mm)",
        "bill_depth_mm": "Bill Depth (mm)"
    }
)

for i, trace in enumerate(fig.data):
    if trace.diagonal:
        fig.data[i].update(opacity=0.7)

fig.update_traces(marker=dict(size=7, opacity=0.6))

fig.show()

"""**Q7: Find the maximum flipper_length_mm for each combination of species and island. Which species has the longest flippers on each island? (code)**"""

# Group by species and island, then find the maximum flipper length
max_flipper_length = data.groupby(['species', 'island'])['flipper_length_mm'].max().reset_index()

# Find the species with the longest flippers for each island
max_flipper_species = max_flipper_length.loc[max_flipper_length.groupby('island')['flipper_length_mm'].idxmax()]


print(max_flipper_species)


fig = px.bar(max_flipper_species,
             x='island',
             y='flipper_length_mm',
             color='species',
             title='Species with the Longest Flippers on Each Island',
             labels={'flipper_length_mm': 'Maximum Flipper Length (mm)', 'island': 'Island'},
             category_orders={'island': max_flipper_species['island'].unique().tolist()})

fig.update_layout(barmode='group')

fig.show()

"""**Q8: Perform z-score normalization on this dataset. (code)**

The Z-score is calculated using the formula:

Z= (X−μ) ÷ σ

Where:

X is the data point,

μ is the mean of the column,

σ is the standard deviation of the column.
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Select the numeric columns for Z-score normalization (excluding non-numeric columns)
numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Apply Z-score normalization
data[numeric_columns] = scaler.fit_transform(data[numeric_columns])

print(data.head)

